[{"path":"index.html","id":"about-this-book","chapter":"About this book","heading":"About this book","text":"Targeted Learning R: Causal Data Science tlverse Software\nEcosystem open source, reproducible electronic handbook applying \nTargeted Learning methodology practice using tlverse software\necosystem. work currently early draft\nphase available facilitate input community. view \ncontribute available content, consider visiting GitHub\nrepository.\n","code":""},{"path":"index.html","id":"outline","chapter":"About this book","heading":"0.1 Outline","text":"contents handbook meant serve reference guide \ndevelopers interested using tlverse implement novel estimators\nmethodological improvements related Targeted Learning. \ngoal tlverse make easier methods researchers quickly implement\ntest (Monte Carlo simulation) innovations. done ,\nhope make tlverse-based implementations available\npeople interested applying methods going forward. believe \nbest way create maintainable continually improving\nset implementations Targeted Learning.(evolving) set materials includes:MotivationIntroduction tlverse software\necosystemComputation graphs Parallelization delayed\npackageCross-validation origami\npackageEnsemble machine learning \nsl3 packageTargeted Learning \ntmle3 packageMonte Carlo simulation \ntmle3sim packageHighly Adaptive Lasso \nhal9001 packageHow contributeCode quality (Style, Documentation, Testing)encourage reference corresponding chapters tlverse handbook applied perspective end users use tools research.","code":""},{"path":"index.html","id":"about-the-authors","chapter":"About this book","heading":"About the authors","text":"","code":""},{"path":"index.html","id":"jeremy-coyle","chapter":"About this book","heading":"Jeremy Coyle","text":"Jeremy Coyle, PhD, consulting data scientist statistical programmer,\ncurrently leading software development effort produced \ntlverse ecosystem R packages related software tools. Jeremy earned \nPhD Biostatistics UC Berkeley 2016, primarily supervision\nAlan Hubbard.","code":""},{"path":"index.html","id":"nima-hejazi","chapter":"About this book","heading":"Nima Hejazi","text":"Nima Hejazi PhD candidate biostatistics, working collaborative\ndirection Mark van der Laan Alan Hubbard. Nima affiliated UC\nBerkeley’s Center Computational Biology NIH Biomedical Big Data training\nprogram, well Fred Hutchinson Cancer Research Center. Previously,\nearned MA Biostatistics BA (majors Molecular Cell\nBiology, Psychology, Public Health), UC Berkeley. research\ninterests fall intersection causal inference machine learning,\ndrawing ideas non/semi-parametric estimation large, flexible\nstatistical models develop efficient robust statistical procedures \nevaluating complex target estimands observational randomized studies.\nParticular areas current emphasis include mediation/path analysis,\noutcome-dependent sampling designs, targeted loss-based estimation, vaccine\nefficacy trials. Nima also passionate statistical computing open\nsource software development applied statistics.","code":""},{"path":"index.html","id":"ivana-malenica","chapter":"About this book","heading":"Ivana Malenica","text":"Ivana Malenica PhD student biostatistics advised Mark van der Laan.\nIvana currently fellow Berkeley Institute Data Science, \nserving NIH Biomedical Big Data Freeport-McMoRan Genomic Engine fellow.\nearned Master’s Biostatistics Bachelor’s Mathematics, \nspent time Translational Genomics Research Institute. broadly,\nresearch interests span non/semi-parametric theory, probability theory,\nmachine learning, causal inference high-dimensional statistics. \ncurrent work involves complex dependent settings (dependence time \nnetwork) adaptive sequential designs.","code":""},{"path":"index.html","id":"rachael-phillips","chapter":"About this book","heading":"Rachael Phillips","text":"Rachael Phillips PhD student biostatistics, advised Alan Hubbard \nMark van der Laan. MA Biostatistics, BS Biology, BA \nMathematics. student targeted learning causal inference, Rachael’s\nresearch focuses statistical estimation inference realistic\nstatistical models. current projects involve personalized online machine\nlearning EHR streaming data vital signs, automated learning \nhighly adaptive lasso, causal effect estimation community-level\ninterventions. also working FDA-funded project led Dr. Susan\nGruber, Targeted Learning Framework Causal Effect Estimation Using\nReal-World Data. Rachael active contributor hal9001 sl3\nR packages tlverse.","code":""},{"path":"index.html","id":"repro","chapter":"About this book","heading":"0.2 Reproduciblity with the tlverse","text":"tlverse software ecosystem growing collection packages, several \nquite early software lifecycle. team best \nmaintain backwards compatibility. work reaches completion, \nspecific versions tlverse packages used archived tagged \nproduce .book written using bookdown, complete\nsource available GitHub.\nversion book built R version 4.1.1 (2021-08-10),\npandoc version 2.7.3, \nfollowing packages:","code":""},{"path":"index.html","id":"learn","chapter":"About this book","heading":"0.3 Learning resources","text":"effectively utilize handbook, reader need fully trained\nstatistician begin understanding applying methods. However, \nhighly recommended reader understanding basic statistical\nconcepts confounding, probability distributions, confidence intervals,\nhypothesis tests, regression. Advanced knowledge mathematical statistics\nmay useful necessary. Familiarity R programming\nlanguage essential. also recommend understanding introductory\ncausal inference.learning R programming language recommend following (free)\nintroductory resources:Software Carpentry’s Programming \nRSoftware Carpentry’s R Reproducible Scientific\nAnalysisGarret Grolemund Hadley Wickham’s R Data\nScienceFor general introduction causal inference, recommendMiguel . Hernán James M. Robins’ Causal Inference: ,\n2021Jason . Roy’s Crash Course Causality: Inferring Causal Effects \nObservational Data \nCoursera","code":""},{"path":"index.html","id":"setup","chapter":"About this book","heading":"0.4 Setup instructions","text":"","code":""},{"path":"index.html","id":"r-and-rstudio","chapter":"About this book","heading":"0.4.1 R and RStudio","text":"R RStudio separate downloads installations. R \nunderlying statistical computing environment. RStudio graphical integrated\ndevelopment environment (IDE) makes using R much easier \ninteractive. need install R install RStudio.","code":""},{"path":"index.html","id":"windows","chapter":"About this book","heading":"0.4.1.1 Windows","text":"","code":""},{"path":"index.html","id":"if-you-already-have-r-and-rstudio-installed","chapter":"About this book","heading":"0.4.1.1.1 If you already have R and RStudio installed","text":"Open RStudio, click “Help” > “Check updates”. new version \navailable, quit RStudio, download latest version RStudio.check version R using, start RStudio first thing\nappears console indicates version R \nrunning. Alternatively, can type sessionInfo(), also display\nversion R running. Go CRAN\nwebsite check whether \nrecent version available. , please download install . \ncan check \ninformation remove old versions system \nwish .","code":""},{"path":"index.html","id":"if-you-dont-have-r-and-rstudio-installed","chapter":"About this book","heading":"0.4.1.1.2 If you don’t have R and RStudio installed","text":"Download R \nCRAN website.Run .exe file just downloadedGo RStudio download pageUnder Installers select RStudio x.yy.zzz - Windows\nXP/Vista/7/8 (x, y, z represent version numbers)Double click file install itOnce ’s installed, open RStudio make sure works don’t get \nerror messages.","code":""},{"path":"index.html","id":"macos-mac-os-x","chapter":"About this book","heading":"0.4.1.2 macOS / Mac OS X","text":"","code":""},{"path":"index.html","id":"if-you-already-have-r-and-rstudio-installed-1","chapter":"About this book","heading":"0.4.1.2.1 If you already have R and RStudio installed","text":"Open RStudio, click “Help” > “Check updates”. new version \navailable, quit RStudio, download latest version RStudio.check version R using, start RStudio first thing\nappears terminal indicates version R running.\nAlternatively, can type sessionInfo(), also display \nversion R running. Go CRAN\nwebsite check whether \nrecent version available. , please download install .","code":""},{"path":"index.html","id":"if-you-dont-have-r-and-rstudio-installed-1","chapter":"About this book","heading":"0.4.1.2.2 If you don’t have R and RStudio installed","text":"Download R \nCRAN website.Select .pkg file latest R versionDouble click downloaded file install RIt also good idea install XQuartz (needed\npackages)Go RStudio download\npageUnder Installers select RStudio x.yy.zzz - Mac OS X 10.6+ (64-bit)\n(x, y, z represent version numbers)Double click file install RStudioOnce ’s installed, open RStudio make sure works don’t get \nerror messages.","code":""},{"path":"index.html","id":"linux","chapter":"About this book","heading":"0.4.1.3 Linux","text":"Follow instructions distribution\nCRAN, provide information\nget recent version R common distributions. \ndistributions, use package manager (e.g., Debian/Ubuntu run\nsudo apt-get install r-base, Fedora sudo yum install R), \ndon’t recommend approach versions provided \nusually date. case, make sure least R 3.3.1.Go RStudio download\npageUnder Installers select version matches distribution, \ninstall preferred method (e.g., Debian/Ubuntu sudo dpkg -rstudio-x.yy.zzz-amd64.deb terminal).’s installed, open RStudio make sure works don’t get \nerror messages.setup instructions adapted written Data Carpentry: R\nData Analysis Visualization Ecological\nData.","code":""},{"path":"why-we-need-yet-another-targeted-learning-package.html","id":"why-we-need-yet-another-targeted-learning-package","chapter":"1 Why we need yet another Targeted Learning package","heading":"1 Why we need yet another Targeted Learning package","text":"","code":""},{"path":"tlverse.html","id":"tlverse","chapter":"2 Welcome to the tlverse","heading":"2 Welcome to the tlverse","text":"Updated: 2021-09-13","code":""},{"path":"tlverse.html","id":"learning-objectives","chapter":"2 Welcome to the tlverse","heading":"Learning Objectives","text":"chapter introduces tlverse software ecosystem, includingUnderstanding tlverse ecosystem conceptually.Identifying core components tlverse.Installing tlverse R packages.Understanding Targeted Learning roadmap.Learning WASH Benefits example data.","code":""},{"path":"tlverse.html","id":"what-is-the-tlverse","chapter":"2 Welcome to the tlverse","heading":"What is the tlverse?","text":"tlverse new framework Targeted Learning R, inspired \ntidyverse ecosystem R packages.analogy tidyverse:tidyverse opinionated collection R packages designed data\nscience. packages share underlying design philosophy, grammar, data\nstructures., tlverse isan opinionated collection R packages Targeted Learningsharing underlying philosophy, grammar, set data structures","code":""},{"path":"tlverse.html","id":"anatomy-of-the-tlverse","chapter":"2 Welcome to the tlverse","heading":"Anatomy of the tlverse","text":"Targeted Learning methods targeted maximum likelihood (minimum\nloss-based) estimators (TMLEs). construction Targeted Learning\nestimator proceeds two-stage process:Flexibly learning particular components data-generating distribution\nmacchine learning (e.g., Super Learning), resulting initial\nestimates nuisance parameters.Use parametric model-based update via maximum likelihood estimation\n(.e., MLE), incorporating initial estimates produced prior step.packages making core components tlverse software ecosystem,\nsl3 tmle3, address two goals, respectively. Together, \ngeneral functionality exposed allows one build specific TMLEs\ntailored exactly particular estimation problem.software packages make core tlverse aresl3: Modern Super Machine Learning\n? modern object-oriented re-implementation Super Learner\nalgorithm, employing recently developed paradigms R programming.\n? design leverages modern ideas faster computation, \neasily extensible forward-looking, forms one cornerstones \ntlverse.\n? modern object-oriented re-implementation Super Learner\nalgorithm, employing recently developed paradigms R programming.? design leverages modern ideas faster computation, \neasily extensible forward-looking, forms one cornerstones \ntlverse.tmle3: Engine Targeted Learning\n? generalized framework simplifies Targeted Learning \nidentifying implementing series common statistical estimation\nprocedures.\n? common interface engine accommodates current algorithmic\napproaches Targeted Learning yet remains flexible enough engine \npower implementation emerging statistical techniques \ndeveloped.\n? generalized framework simplifies Targeted Learning \nidentifying implementing series common statistical estimation\nprocedures.? common interface engine accommodates current algorithmic\napproaches Targeted Learning yet remains flexible enough engine \npower implementation emerging statistical techniques \ndeveloped.Beyond engines provide driving force behind tlverse, \nsupporting packages play important roles background:origami: Generalized Framework \nCross-Validation (Coyle Hejazi 2018)\n? generalized framework flexible cross-validation.\n? Cross-validation key part ensuring error estimates honest\npreventing overfitting. essential part Super\nLearner ensemble modeling algorithm construction Targeted\nLearning estimators.\n? generalized framework flexible cross-validation.? Cross-validation key part ensuring error estimates honest\npreventing overfitting. essential part Super\nLearner ensemble modeling algorithm construction Targeted\nLearning estimators.delayed: Parallelization Framework \nDependent Tasks\n? framework delayed computations (.e., futures) based task\ndependencies.\n? Efficient allocation compute resources essential deploying\ncomputationally intensive algorithms large scale.\n? framework delayed computations (.e., futures) based task\ndependencies.? Efficient allocation compute resources essential deploying\ncomputationally intensive algorithms large scale.key principle tlverse extensibility. , software\necosystem aims support development new Targeted Learning estimators \nreaching maturity. achieve degree flexibility, follow \nmodel implementing new classes estimators, distinct causal inference\nproblems, separate packages, use core machinery provided \nsl3 tmle3 packages currently three examples:tmle3mopttx: Optimal Treatments\ntlverse\n? Learn optimal rule estimate mean outcome rule.\n? Optimal treatments powerful tool precision healthcare \nsettings one-size-fits-treatment approach \nappropriate.\n? Learn optimal rule estimate mean outcome rule.? Optimal treatments powerful tool precision healthcare \nsettings one-size-fits-treatment approach \nappropriate.tmle3shift: Stochastic Shift\nInterventions tlverse\n? Stochastic shift interventions continuous-valued treatments.\n? treatment variables binary categorical. Estimating \ntotal effects intervening continuous-valued treatments provides way\nprobe effect changes shifts treatment variable.\n? Stochastic shift interventions continuous-valued treatments.? treatment variables binary categorical. Estimating \ntotal effects intervening continuous-valued treatments provides way\nprobe effect changes shifts treatment variable.tmle3mediate: Causal Mediation\nAnalysis tlverse\n? Techniques evaluating direct indirect effects \ntreatments mediating variables.\n? Evaluating total effect treatment provide\ninformation pathways may operate. mediating\nvariables collected, one can instead evaluate direct indirect\neffect parameters speak action mechanism treatment.\n? Techniques evaluating direct indirect effects \ntreatments mediating variables.? Evaluating total effect treatment provide\ninformation pathways may operate. mediating\nvariables collected, one can instead evaluate direct indirect\neffect parameters speak action mechanism treatment.","code":""},{"path":"tlverse.html","id":"installtlverse","chapter":"2 Welcome to the tlverse","heading":"2.1 Installation","text":"tlverse ecosystem packages currently hosted \nhttps://github.com/tlverse, yet CRAN. \ncan use usethis package install :tlverse depends large number packages also hosted\nGitHub. , may see following error:just means R tried install many packages GitHub \nshort window. fix , need tell R use GitHub \nuser (’ll need GitHub user account). Follow two steps:Type usethis::browse_github_pat() R console, direct\nGitHub’s page create New Personal Access Token (PAT).Type usethis::browse_github_pat() R console, direct\nGitHub’s page create New Personal Access Token (PAT).Create PAT simply clicking “Generate token” bottom page.Create PAT simply clicking “Generate token” bottom page.Copy PAT, long string lowercase letters numbers.Copy PAT, long string lowercase letters numbers.Type usethis::edit_r_environ() R console, open \n.Renviron file source window RStudio.\n.Renviron file pop-calling\nusethis::edit_r_environ(); try inputting\nSys.setenv(GITHUB_PAT = \"yourPAT\"), replacing PAT inside \nquotes. error, skip step 8.\nType usethis::edit_r_environ() R console, open \n.Renviron file source window RStudio..Renviron file pop-calling\nusethis::edit_r_environ(); try inputting\nSys.setenv(GITHUB_PAT = \"yourPAT\"), replacing PAT inside \nquotes. error, skip step 8..Renviron file, type GITHUB_PAT= paste PAT \nequals symbol space..Renviron file, type GITHUB_PAT= paste PAT \nequals symbol space..Renviron file, press enter key ensure .Renviron\nends new line..Renviron file, press enter key ensure .Renviron\nends new line.Save .Renviron file. example shows syntax \nlook.Save .Renviron file. example shows syntax \nlook.Restart R. can restart R via drop-menu RStudio’s “Session”\ntab, located top RStudio interface. \nrestart R changes take effect!following steps, able successfully install \npackage threw error .","code":"\ninstall.packages(\"devtools\")\ndevtools::install_github(\"tlverse/tlverse\")Error: HTTP error 403.\n  API rate limit exceeded for 71.204.135.82. (But here's the good news:\n  Authenticated requests get a higher rate limit. Check out the documentation\n  for more details.)\n\n  Rate limit remaining: 0/60\n  Rate limit reset at: 2019-03-04 19:39:05 UTC\n\n  To increase your GitHub API rate limit\n  - Use `usethis::browse_github_pat()` to create a Personal Access Token.\n  - Use `usethis::edit_r_environ()` and add the token as `GITHUB_PAT`.\nGITHUB_PAT <- yourPAT"},{"path":"computation-graphs-with-delayed.html","id":"computation-graphs-with-delayed","chapter":"3 Computation Graphs with delayed","heading":"3 Computation Graphs with delayed","text":"Jeremy Coyle","code":""},{"path":"computation-graphs-with-delayed.html","id":"intro","chapter":"3 Computation Graphs with delayed","heading":"3.1 Intro","text":"","code":""},{"path":"computation-graphs-with-delayed.html","id":"architecture","chapter":"3 Computation Graphs with delayed","heading":"3.2 Architecture","text":"","code":""},{"path":"computation-graphs-with-delayed.html","id":"other","chapter":"3 Computation Graphs with delayed","heading":"3.3 Other","text":"","code":""},{"path":"computation-graphs-with-delayed.html","id":"previous-documentation","chapter":"3 Computation Graphs with delayed","heading":"3.4 Previous Documentation","text":"R supports range options parallelize computation. overview, see\nHPC Task\nView \nCRAN. general, options work extremely well problems \nembarassingly parallel, support procedures parallel\nlapply calls parallel loops – essentially map operations.\nHowever, easy way parallelize dependent tasks R.contrast, Python language excellent framework exactly \npurpose – dask. dask makes easy \nbuild graph interdependent tasks execute parallel \norder optimizes performance (???). present package seeks \nreproduce subset functionality R, specifically \ndelayed module. \nparallelize across tasks, leverage excellent\nfuture package (???).power delayed framework best appreciated demonstrated \nexample.","code":""},{"path":"computation-graphs-with-delayed.html","id":"example","chapter":"3 Computation Graphs with delayed","heading":"3.5 Example","text":"two primary ways generate Delayed objects R via delayed\ndelayed_fun functions.delayed used delay expressions…delayed_fun wraps function returns Delayed resultsThese elements functionality delayed substantially similar \nfacilities already offered future package. delayed diverges \nfuture offereing ability chain Delayed objects together. \nexample:can visualize dependency structure delayed tasks calling\nplot resultant Delayed object:","code":"library(delayed)\n# delay a simple expression\ndelayed_object <- delayed(3 + 4)\nprint(delayed_object)\n[1] \"delayed(3 + 4)\"\n# compute its result\ndelayed_object$compute()\n[1] 7# delay a function\nx2 <- function(x) {x * x}\ndelayed_x2 <- delayed_fun(x2)\n\n# calling it returns a delayed call\ndelayed_object <- delayed_x2(4)\nprint(delayed_object)\n[1] \"delayed(x2(x = 4))\"\n# again, we can compute its result\ndelayed_object$compute()\n[1] 16# delay a simple expression\ndelayed_object_7 <- delayed(3 + 4)\n\n# and another\ndelayed_object_3 <- delayed(1 + 2)\n\n# delay a function for addition\nadder <- function(x, y){x + y}\ndelayed_adder <- delayed_fun(adder)\n\n# but now, use one delayed as input to another\nchained_delayed_10 <- delayed_adder(delayed_object_7, delayed_object_3)\n\n# We can still compute its result.\nchained_delayed_10$compute()\n[1] 10\nplot(chained_delayed_10)"},{"path":"computation-graphs-with-delayed.html","id":"parallelization","chapter":"3 Computation Graphs with delayed","heading":"3.6 Parallelization","text":"Now ’ve elementary look functionality offered delayed,\nmay take look parallelize dependent computations – core\nproblem addressed package. can easily parallelize across dependency\nstructures specifying future plan. Let’s try outThe illustrates typical lifecycle delayed task. procedures\nstart state \"waiting\", means given task depends \ndelayed tasks yet complete. task question \ndelayed dependencies – dependencies become resolved – task\ntransitions \"ready\" state. means run soon worker\navailable process task. task assigned worker, \nstate task changes \"running\"; processing task \ncomplete, finally marked \"resolved\".","code":"library(future)\nplan(multicore, workers = 2)\n\n# re-define the delayed object from above\ndelayed_object_7 <- delayed(3 + 4)\ndelayed_object_3 <- delayed(1 + 2)\nchained_delayed_10 <- delayed_adder(delayed_object_7, delayed_object_3)\n\n# compute it using the future plan (two multicore workers), verbose mode lets\n# us see the computation order\nchained_delayed_10$compute(nworkers = 2, verbose = TRUE)\n[1] 10"},{"path":"computation-graphs-with-delayed.html","id":"future-work","chapter":"3 Computation Graphs with delayed","heading":"3.7 Future Work","text":"","code":""},{"path":"computation-graphs-with-delayed.html","id":"scheduling-tasks","chapter":"3 Computation Graphs with delayed","heading":"3.7.1 Scheduling Tasks","text":"multiple tasks simulatenously \"ready\", scheduler must decide\nassign next available worker. Currently, scheduler simply\nprioritizes tasks likely result tasks becoming “ready”. \nfuture, plan build advanced scheduling features, similar \navailable dask library. overview functionality described\n: https://distributed.readthedocs.io/en/latest/scheduling-policies.html","code":""},{"path":"computation-graphs-with-delayed.html","id":"distributed-data","chapter":"3 Computation Graphs with delayed","heading":"3.7.2 Distributed Data","text":"Another key features dask data\nlocality. ,\ndata present workers need given task, \nshared workers necessary. Tasks prioritized workers\nnecessary components. begun implement similar\nframework, though work remains incomplete.","code":""},{"path":"cross-validation-with-origami.html","id":"cross-validation-with-origami","chapter":"4 Cross-validation with origami","heading":"4 Cross-validation with origami","text":"Jeremy Coyle","code":""},{"path":"cross-validation-with-origami.html","id":"intro-1","chapter":"4 Cross-validation with origami","heading":"4.1 Intro","text":"","code":""},{"path":"cross-validation-with-origami.html","id":"architecture-1","chapter":"4 Cross-validation with origami","heading":"4.2 Architecture","text":"","code":""},{"path":"cross-validation-with-origami.html","id":"other-1","chapter":"4 Cross-validation with origami","heading":"4.3 Other","text":"","code":""},{"path":"cross-validation-with-origami.html","id":"previous-documentation-1","chapter":"4 Cross-validation with origami","heading":"4.4 Previous Documentation","text":"","code":""},{"path":"cross-validation-with-origami.html","id":"general-workflow","chapter":"4 Cross-validation with origami","heading":"4.5 General workflow","text":"Generally, cross_validate usage mirror workflow example.\nFirst, user must define folds function operates fold.\npassed cross_validate, function map function\nacross folds, combine results reasonable way. details \nstep process given .","code":""},{"path":"cross-validation-with-origami.html","id":"define-folds","chapter":"4 Cross-validation with origami","heading":"4.5.1 Define folds","text":"folds object passed cross_validate list folds. lists can\ngenerated using make_folds function. fold consists list\ntraining index vector, validation index vector, fold_index\n(order list folds). function supports variety \ncross-validation schemes including v-fold bootstrap cross-validation \nwell time series methods like “Rolling Window”. can balance across levels \nvariable (stratify_ids), can also keep observations \nindependent unit together (cluster_ids). See documentation make_folds\nfunction details supported cross-validation schemes arguments.","code":""},{"path":"cross-validation-with-origami.html","id":"define-fold-function","chapter":"4 Cross-validation with origami","heading":"4.5.2 Define fold function","text":"cv_fun argument cross_validate function perform \noperation fold. first argument function must fold,\nreceive individual fold object operate . Additional arguments\ncan passed cv_fun using ... argument cross_validate. Within\nfunction, convenience functions training, validation \nfold_index can return various components fold object. \nretrieving fold object calling environment. can also \nspecified directly. training validation passed object, \nindex sensible way. instance, vector, index\nvector directly. data.frame matrix, index rows.\nallows user easily partition data training validation sets.\nfold function must return named list results containing whatever\nfold-specific outputs generated.","code":""},{"path":"cross-validation-with-origami.html","id":"apply-cross_validate","chapter":"4 Cross-validation with origami","heading":"4.5.3 Apply cross_validate","text":"defining folds, cross_validate can used map cv_fun across\nfolds using future_lapply. means can easily parallelized\nspecifying parallelization scheme (.e., plan). See future\npackage details.application cross_validate generates list results. described\n, call cv_fun returns list results, different\nelements type result care . main loop generates list\nindividual lists results (sort “meta-list”). “meta-list”\ninverted one element per result type (\nlist results fold). default, combine_results used \ncombine results type lists.instance, mtcars example, results type lists contains one\ncoef data.frame fold. rbinded together form one\ndata.frame containing coefs folds different rows. \nresults combined determined automatically examining data types\nresults first fold. can modified specifying list \narguments .combine_control. See help combine_results \ndetails. cases, defaults suffice.","code":""},{"path":"super-machine-learning-with-sl3.html","id":"super-machine-learning-with-sl3","chapter":"5 Super (Machine) Learning with sl3","heading":"5 Super (Machine) Learning with sl3","text":"Jeremy Coyle","code":""},{"path":"super-machine-learning-with-sl3.html","id":"intro-2","chapter":"5 Super (Machine) Learning with sl3","heading":"5.1 Intro","text":"","code":""},{"path":"super-machine-learning-with-sl3.html","id":"architecture-2","chapter":"5 Super (Machine) Learning with sl3","heading":"5.2 Architecture","text":"","code":""},{"path":"super-machine-learning-with-sl3.html","id":"other-2","chapter":"5 Super (Machine) Learning with sl3","heading":"5.3 Other","text":"","code":""},{"path":"super-machine-learning-with-sl3.html","id":"previous-documentation-2","chapter":"5 Super (Machine) Learning with sl3","heading":"5.4 Previous Documentation","text":"","code":""},{"path":"super-machine-learning-with-sl3.html","id":"introduction","chapter":"5 Super (Machine) Learning with sl3","heading":"5.5 Introduction","text":"guide describes process implementing learner class new\nmachine learning algorithm. writing learner class favorite machine\nlearning algorithm, able use places \notherwise use sl3 learners, including Pipelines, Stacks, \nSuper Learner. done best streamline process creating new\nsl3 learners.diving defining new learner, likely helpful read\nbackground material. haven’t already read , “Modern Machine\nLearning R” vignette good introduction sl3\npackage ’s underlying architecture. \nR6\ndocumentation help understand R6 classes defined. \naddition, help files \nsl3_Task \nLrnr_base good\nresources objects can used. ’re interested defining\nlearners fit sub-learners, reading documentation \ndelayed package \nhelpful.following sections, introduce review template new sl3\nlearner, describing sections can used define new learner.\nfollowed discussion important task documenting \ntesting new learner. Finally, conclude explaining can add\nlearner sl3 others may make use .","code":"\nlibrary(sl3)"},{"path":"super-machine-learning-with-sl3.html","id":"learner-template","chapter":"5 Super (Machine) Learning with sl3","heading":"5.6 Learner Template","text":"sl3 provides template learner use defining new learners. can\nmake copy template work invoking write_learner_template:Let’s take look template:template comments indicating details specific learner\n’re trying implement filled . next section, \ndiscuss details .","code":"\n## Not run:\nwrite_learner_template(\"path/to/write/Learner_template.R\")\n##' Template of a \\code{sl3} Learner.\n##'\n##' This is a template for defining a new learner.\n##' This can be copied to a new file using \\code{\\link{write_learner_template}}.\n##' The remainder of this documentation is an example of how you might write documentation for your new learner.\n##' This learner uses \\code{\\link[my_package]{my_ml_fun}} from \\code{my_package} to fit my favorite machine learning algorithm.\n##'\n##' @docType class\n##' @importFrom R6 R6Class\n##' @export\n##' @keywords data\n##' @return Learner object with methods for training and prediction. See \\code{\\link{Lrnr_base}} for documentation on learners.\n##' @format \\code{\\link{R6Class}} object.\n##' @family Learners\n##'\n##' @section Parameters:\n##' \\describe{\n##'   \\item{\\code{param_1=\"default_1\"}}{ This parameter does something.\n##'   }\n##'   \\item{\\code{param_2=\"default_2\"}}{ This parameter does something else.\n##'   }\n##'   \\item{\\code{...}}{ Other parameters passed directly to \\code{\\link[my_package]{my_ml_fun}}. See its documentation for details.\n##'   }\n##' }\n##'\n##' @section Methods:\n##' \\describe{\n##' \\item{\\code{special_function(arg_1)}}{\n##'   My learner is special so it has a special function.\n##'\n##'   \\itemize{\n##'     \\item{\\code{arg_1}: A very special argument.\n##'    }\n##'   }\n##'   }\n##' }\nLrnr_template <- R6Class(\n  classname = \"Lrnr_template\", inherit = Lrnr_base,\n  portable = TRUE, class = TRUE,\n  # Above, you should change Lrnr_template (in both the object name and the classname argument)\n  # to a name that indicates what your learner does\n  public = list(\n    # you can define default parameter values here\n    # if possible, your learner should define defaults for all required parameters\n    initialize = function(param_1 = \"default_1\", param_2 = \"default_2\", ...) {\n      # this captures all parameters to initialize and saves them as self$params\n      params <- args_to_list()\n      super$initialize(params = params, ...)\n    },\n\n    # you can define public functions that allow your learner to do special things here\n    # for instance glm learner might return prediction standard errors\n    special_function = function(arg_1) {\n    }\n  ),\n  private = list(\n    # list properties your learner supports here.\n    # Use sl3_list_properties() for a list of options\n    .properties = c(\"\"),\n\n    # list any packages required for your learner here.\n    .required_packages = c(\"my_package\"),\n\n    # .train takes task data and returns a fit object that can be used to generate predictions\n    .train = function(task) {\n      # generate an argument list from the parameters that were\n      # captured when your learner was initialized.\n      # this allows users to pass arguments directly to your ml function\n      args <- self$params\n\n      # get outcome variable type\n      # preferring learner$params$outcome_type first, then task$outcome_type\n      outcome_type <- self$get_outcome_type(task)\n      # should pass something on to your learner indicating outcome_type\n      # e.g. family or objective\n\n      # add task data to the argument list\n      # what these arguments are called depends on the learner you are wrapping\n      args$x <- as.matrix(task$X_intercept)\n      args$y <- outcome_type$format(task$Y)\n\n      # only add arguments on weights and offset\n      # if those were specified when the task was generated\n      if (task$has_node(\"weights\")) {\n        args$weights <- task$weights\n      }\n\n      if (task$has_node(\"offset\")) {\n        args$offset <- task$offset\n      }\n\n      # call a function that fits your algorithm\n      # with the argument list you constructed\n      fit_object <- call_with_args(my_ml_fun, args)\n\n      # return the fit object, which will be stored\n      # in a learner object and returned from the call\n      # to learner$predict\n      return(fit_object)\n    },\n\n    # .predict takes a task and returns predictions from that task\n    .predict = function(task = NULL) {\n      self$training_task\n      self$training_outcome_type\n      self$fit_object\n\n      predictions <- predict(self$fit_object, task$X)\n      return(predictions)\n    }\n  )\n)"},{"path":"super-machine-learning-with-sl3.html","id":"defining-your-learner","chapter":"5 Super (Machine) Learning with sl3","heading":"5.7 Defining your Learner","text":"","code":""},{"path":"super-machine-learning-with-sl3.html","id":"learner-name-and-class","chapter":"5 Super (Machine) Learning with sl3","heading":"5.7.1 Learner Name and Class","text":"top template, define object Lrnr_template set\nclassname = \"Lrnr_template\". modify match name \nnew learner, also match name corresponding R file. Note\nname prefixed Lrnr_ use\nsnake_case.","code":""},{"path":"super-machine-learning-with-sl3.html","id":"publicinitialize","chapter":"5 Super (Machine) Learning with sl3","heading":"5.7.2 public$initialize","text":"function defines constructor learner, stores \narguments () provided user calls\nmake_learner(Lrnr_your_learner, ...). can also provide default parameter\nvalues, just template param_1 = \"default_1\", \nparam_2 = \"default_2\". parameters used newly defined learners\ndefaults whenever possible. allow users use \nlearner without figure reasonable parameter values might .\nParameter values documented; see section \ndocumentation details.","code":""},{"path":"super-machine-learning-with-sl3.html","id":"publicspecial_functions","chapter":"5 Super (Machine) Learning with sl3","heading":"5.7.3 public$special_functions","text":"can course define functions things learner can . \npublic functions like special_function defined example.\ndocumented; see section documentation\ndetails.","code":""},{"path":"super-machine-learning-with-sl3.html","id":"private.properties","chapter":"5 Super (Machine) Learning with sl3","heading":"5.7.4 private$.properties","text":"field defines properties supported learner. may include\ndifferent outcome types supported, offsets weights, amongst many\npossibilities. see list properties supported/used least\none learner, may invoke sl3_list_properties:","code":"sl3_list_properties()\n [1] \"binomial\"      \"categorical\"   \"continuous\"    \"cv\"           \n [5] \"density\"       \"h2o\"           \"ids\"           \"importance\"   \n [9] \"offset\"        \"preprocessing\" \"sampling\"      \"screener\"     \n[13] \"timeseries\"    \"weights\"       \"wrapper\"      "},{"path":"super-machine-learning-with-sl3.html","id":"private.required_packages","chapter":"5 Super (Machine) Learning with sl3","heading":"5.7.5 private$.required_packages","text":"field defines R packages required learner work properly.\nloaded object new learner class initialized.","code":""},{"path":"super-machine-learning-with-sl3.html","id":"user-interface-for-learners","chapter":"5 Super (Machine) Learning with sl3","heading":"5.7.6 User Interface for Learners","text":"’ve used sl3 , may noticed users \ninstructed use learner$train, learner$predict, learner$chain, \ntrain, generate predictions, generate chained task given learner\nobject, respectively, template implement methods. Instead,\ntemplate implements private methods called .train, .predict, \n.chain. specifics methods explained ; however, \nhelpful first understand two sets methods related. risk\ncomplicating things , worth noting actually \nthird set methods (learner$base_train, learner$base_predict, \nlearner$base_chain) may aware., happens user calls learner$train? method generates \ndelayed object using delayed_learner_train function, computes\ndelayed object. turn, delayed_learner_train defines delayed\ncomputation calls base_train, user-facing function can used \ntrain tasks without using facilities delayed package. base_train\nvalidates user input, turn calls private$.train. \nprivate$.train returns fit_object, base_train takes fit object,\ngenerates learner fit object, returns user.call learner$train involves three separate training methods:user-facing learner$train – trains learner manner can \nparallelized using delayed, calls ...... user-facing learner$base_train validates user input, \ncalls ...... internal private$.train, actual work fitting\nlearner returning fit object.logic user-facing learner$train learner$base_train defined\nLrnr_base base class shared across learners. , \nmethods need reimplemented individual learners. contrast,\nprivate$.train contains behavior specific individual\nlearner reimplemented level individual learner.\nSince learner$base_train use delayed, may helpful use \ndebugging training code new learner. program flow used \nprediction chaining analogous.","code":""},{"path":"super-machine-learning-with-sl3.html","id":"private.train","chapter":"5 Super (Machine) Learning with sl3","heading":"5.7.7 private$.train","text":"main training function, takes task returns \nfit_object contains information needed generate predictions. \nfit object contain data absolutely necessary, \nincluding excess information create needless inefficiencies. Many learner\nfunctions (like glm) store one copies training data – \nuses unnecessary memory hurt learner performance large sample\nsizes. Thus, copies data removed fit object\nreturned. may make use true_obj_size estimate size\nfit_object. learners, fit_object size grow\nlinearly training sample size. , unexpected, please\ntry reduce size fit_object.time, learner implementing fit using function\nalready exists elsewhere. ’ve built tools facilitate passing\nparameter values directly functions. private$.train function \ntemplate uses common pattern: builds argument list starting \nparameter values using data task, uses call_with_args\ncall my_ml_fun argument list. ’s required learners use\npattern, helpful common case learner \nsimply wrapping underlying my_ml_fun.default, call_with_args pass arguments argument list\nmatched definition function calling. allows \nlearner silently drop irrelevant parameters call my_ml_fun.\nlearners either capture important arguments using dot arguments (...) \npassing important arguments dot arguments secondary\nfunction. cases can handled using other_valid \nkeep_all options call_with_args. former allows list \nvalid arguments latter disables argument filtering altogether.","code":""},{"path":"super-machine-learning-with-sl3.html","id":"private.predict","chapter":"5 Super (Machine) Learning with sl3","heading":"5.7.8 private$.predict","text":"main prediction function, takes task generates\npredictions task using fit_object. predictions \n1-dimensional, coerced vector base_predict.","code":""},{"path":"super-machine-learning-with-sl3.html","id":"private.chain","chapter":"5 Super (Machine) Learning with sl3","heading":"5.7.9 private$.chain","text":"main chaining function. takes task generates chained\ntask (based input task) using given fit_object. method \nimplemented, learner use default chaining behavior, \nreturn new task covariates defined learner’s\npredictions current task.","code":""},{"path":"super-machine-learning-with-sl3.html","id":"advanced-learners-with-sub-learners","chapter":"5 Super (Machine) Learning with sl3","heading":"5.7.10 Advanced: Learners with sub-learners","text":"Generally speaking, sections ’s required \nimplementing new learner sl3 framework. cases, may \ndesirable define learners “sub-learners” learners \ndepend. Examples learners Stack, Pipeline, Lrnr_cv, \nLrnr_sl. order parallelize fitting sub-learners using\ndelayed, learners implement specialized private$.train_sublearners\nmethod calls delayed_learner_train sub-learners, returning \nsingle delayed object , evaluated, returns relevant fit objects\nsub-learners. result call passed second\nargument private$.train method, now function prototype\nprivate$.train(task, trained_sublearners). Learners defined manner\nusually much shorter computation time; predict chain methods\ncurrently parallelized way, although subject change\nfuture., like learners, learner depends sub-learners, two\noptions:Don’t worry parallelizing sub-learners. Simply implement\nprivate$.train discussed , sure call\nsublearner$base_train sublearner$train, avoid nesting calls \ndelayed, may result sub-optimal performance.Implement private$.train_sublearners(task) private$.train(task, trained_sublearners), parallelize sub-learners using delayed. \nsuggest reviewing implementations Stack, Pipeline, Lrnr_cv\nLrnr_sl get better understanding implement parallelized\nsub-learners.either case, careful call sublearner$base_predict \nsublearner$base_chain, instead sublearner$predict sublearner$chain,\nexcept context private$.train_sublearners function, \nuse delayed_learner_fit_predict delayed_learner_fit_chain.","code":""},{"path":"the-tmle-framework-with-tmle3.html","id":"the-tmle-framework-with-tmle3","chapter":"6 The TMLE Framework with tmle3","heading":"6 The TMLE Framework with tmle3","text":"Jeremy Coyle","code":""},{"path":"the-tmle-framework-with-tmle3.html","id":"intro-3","chapter":"6 The TMLE Framework with tmle3","heading":"6.1 Intro","text":"","code":""},{"path":"the-tmle-framework-with-tmle3.html","id":"architecture-3","chapter":"6 The TMLE Framework with tmle3","heading":"6.2 Architecture","text":"","code":""},{"path":"the-tmle-framework-with-tmle3.html","id":"other-3","chapter":"6 The TMLE Framework with tmle3","heading":"6.3 Other","text":"","code":""},{"path":"the-tmle-framework-with-tmle3.html","id":"previous-documentation-3","chapter":"6 The TMLE Framework with tmle3","heading":"6.4 Previous documentation","text":"","code":""},{"path":"the-tmle-framework-with-tmle3.html","id":"introduction-1","chapter":"6 The TMLE Framework with tmle3","heading":"6.5 Introduction","text":"tmle3 package differs previous TMLE software efforts \nattempts directly model key objects defined mathematical \ntheoretical framework Targeted Minimum Loss-Based Estimation (TMLE). ,\nrather focus implementing specific TML estimator, small set \nrelated estimators, focus modeling TMLE framework .Therefore, explicitly define objects model NPSEM, factorized\nlikelihood, counterfactual interventions, parameters, TMLE update\nprocedures. hope , , possible support \nsubstantial subset vast array TML estimators currently present \nliterature, well yet developed. vignette, \ndescribe mathematical objects, software analogs tmle3, \nillustrate motivating example, described . end, describe\nobjects can bundled complete specification TML\nestimation procedure can easily applied end user.","code":""},{"path":"the-tmle-framework-with-tmle3.html","id":"motivating-example","chapter":"6 The TMLE Framework with tmle3","heading":"6.5.1 Motivating Example","text":"use data Collaborative Perinatal Project (CPP), available \nsl3 package. simplify example, define binary intervention\nvariable, parity01 – indicator one children \ncurrent child binary outcome, haz01 – indicator \naverage height age.","code":"\nlibrary(tmle3)\nlibrary(sl3)\ndata(cpp)\ncpp <- cpp[!is.na(cpp[, \"haz\"]), ]\ncpp$parity01 <- as.numeric(cpp$parity > 0)\ncpp[is.na(cpp)] <- 0\ncpp$haz01 <- as.numeric(cpp$haz > 0)"},{"path":"the-tmle-framework-with-tmle3.html","id":"npsem","chapter":"6 The TMLE Framework with tmle3","heading":"6.6 NPSEM","text":"TMLE requires specification Nonparametric Structural Equation Model\n(NPSEM), specifies knowledge relationships variables.start set endogenous variables, \\(X=(X_1,\\ldots,X_J)\\), want\nmodel relationship . \\(X_j\\) least partially observed \ndataset. NPSEM defines variable (\\(X_j\\)) deterministic function\n(\\(f_{X_j}\\)) parent nodes (\\(Pa(X_j)\\)) exogenous random variable\n(\\(U_{X_j}\\)):\\[X_j = f_{X_j}(Pa(X_j), U_{X_j}),\\;\\; j\\\\{1, \\ldots, J\\}\\]exact functional form functions \\(f_{X_j}\\) left unspecified \nstep. priori knowledge functions, can \nspecified likelihood step .","code":""},{"path":"the-tmle-framework-with-tmle3.html","id":"causal-considerations","chapter":"6 The TMLE Framework with tmle3","heading":"6.6.1 Causal Considerations","text":"collection exogenous random variables defined NPSEM \n\\(U = (U_{X_1}, \\ldots, U_{X_J})\\). Typically, non-testable assumptions \njoint distribution \\(U\\) necessary identifiability causal parameters\nstatistical parameters observed data. assumptions \nmanaged tmle3 framework, instead focus statistical\nestimation problem. Therefore, developing tools end users need \nclear additional causal assumptions necessary causal\ninterpretation estimates.","code":""},{"path":"the-tmle-framework-with-tmle3.html","id":"example-1","chapter":"6 The TMLE Framework with tmle3","heading":"6.6.2 Example","text":"case CPP example, use classic point treatment NPSEM \ndefines three nodes: \\(X = (W, , Y)\\), \\(W\\) set baseline covariates,\n\\(\\) exposure interest (parity01), \\(Y\\) outcome interest\n(haz01). define following SCM:\\[W = f_W(U_W)\\]\n\\[= f_A(W, U_A)\\]\n\\[Y = f_Y(W, U_Y)\\]tmle3, done using define_node function node.\ndefine_node allows user specify node_name, columns data\ncomprise node, list parent nodes.Nodes also track information data types variables (continuous,\ncategorical, binomial, etc). , information estimated\nautomatically data. future, node also contain\ninformation censoring indicators, applicable, yet\nimplemented.","code":"\nnpsem <- list(\n  define_node(\"W\", c(\n    \"apgar1\", \"apgar5\", \"gagebrth\", \"mage\",\n    \"meducyrs\", \"sexn\"\n  )),\n  define_node(\"A\", c(\"parity01\"), c(\"W\")),\n  define_node(\"Y\", c(\"haz01\"), c(\"A\", \"W\"))\n)"},{"path":"the-tmle-framework-with-tmle3.html","id":"tmle3_task","chapter":"6 The TMLE Framework with tmle3","heading":"6.6.3 tmle3_Task","text":"tmle3_Task object comprised observed data, NPSEM defined\n:task object contains methods help subset data needed various\nsteps TMLE process:tmle3_Task special kind sl3_Task can used estimate\nfactors likelihood data. process defining estimating \nlikelihood described next.","code":"\ntmle_task <- tmle3_Task$new(cpp, npsem = npsem)# get the outcome node data\nhead(tmle_task$get_tmle_node(\"Y\"))\n[1] 1 1 1 0 0 1\n# get the sl3 task corresponding to an outcome regression\ntmle_task$get_regression_task(\"Y\")\nA sl3 Task with 1441 obs and these nodes:\n$covariates\n         A         W1         W2         W3         W4         W5         W6 \n\"parity01\"   \"apgar1\"   \"apgar5\" \"gagebrth\"     \"mage\" \"meducyrs\"     \"sexn\" \n\n$outcome\n[1] \"haz01\"\n\n$id\nNULL\n\n$weights\nNULL\n\n$offset\nNULL\n\n$time\nNULL"},{"path":"the-tmle-framework-with-tmle3.html","id":"likelihood","chapter":"6 The TMLE Framework with tmle3","heading":"6.7 Likelihood","text":"defined NPSEM, can now define joint likelihood (probability\ndensity function) observed variables \\(X\\):\\[P(X_1, \\ldots, X_J \\D) = \\int_D f_{X_1, \\ldots, X_J}(x_1, \\ldots, x_J)\ndx_1, \\ldots, dx_J\\]can factorized series conditional densities according \nNPSEM:\n\\[f_{X_1, \\ldots, X_J} = \\prod_j^J f_{X_j \\mid Pa(X_j)}(x \\mid Pa(x_j))\\]\\(f_{X_j \\mid Pa(X_j)}\\) conditional pdf (probability mass\nfunction discrete \\(X_j\\)), conditioning set parent nodes \ndefined NPSEM. refer objects likelihood factors.TMLE depends estimates (priori knowledge) functional form \nlikelihood factors. However, factors likelihood always\nnecessary estimation, necessary estimated.","code":""},{"path":"the-tmle-framework-with-tmle3.html","id":"likelihood-factor-objects","chapter":"6 The TMLE Framework with tmle3","heading":"6.7.1 Likelihood Factor Objects","text":"tmle3 models likelihood list likelihood factor objects, \nlikelihood factor object describes either priori knowledge \nestimation strategy corresponding likelihood factor. objects \ninherit LF_base base class, different types depending\nrange estimation strategies priori knowledge \nappropriate.cases, full conditional density particular factor \nnecessary. Instead, conditional mean – much easier quantity estimate –\n’s required. Although conditional means truly likelihood\nfactors, conditional means also modeled using using likelihood factor\nobjects.","code":""},{"path":"the-tmle-framework-with-tmle3.html","id":"lf_emp","chapter":"6 The TMLE Framework with tmle3","heading":"6.7.2 LF_emp","text":"LF_emp represents likelihood factor estimated using nonparametric\nmaximum likelihood estimation (NP-MLE). , probability mass \\(\\frac{1}{n}\\)\nplaced observation observed dataset:\\[f_{X_j}(x_j) = \\frac{1}{n}\\mathbb{}(x_j \\X_{n,j})\\]Going forward, weights used specified, although yet\nsupported. LF_emp supports marginal densities. , conditioning\nset, \\(Pa(X_j)\\) must empty. Therefore, appropriate estimation\nmarginal density baseline covariates.","code":""},{"path":"the-tmle-framework-with-tmle3.html","id":"lf_fit","chapter":"6 The TMLE Framework with tmle3","heading":"6.7.3 LF_fit","text":"LF_fit represents likelihood factor estimated using sl3\nframework. Based learner type used, can fit pmf (binomial \ncategorical data, see sl3_list_learners(\"binomial\") \nsl3_list_learners(\"categorical\") lists), conditional mean (\nlearners), conditional density (e.g., using semiparametric conditional\ndensity estimator via Lrnr_density_semiparametric). LF_fit takes sl3\nlearner object argument, fit data tmle3_Task\nautomatically. Details specifying different kinds learners sl3 may\nfound http://tlverse.org/sl3/articles/intro_sl3.html","code":""},{"path":"the-tmle-framework-with-tmle3.html","id":"specifying-a-priori-knowledge.","chapter":"6 The TMLE Framework with tmle3","heading":"6.7.4 Specifying a priori knowledge.","text":"likelihood factor types, LF_fit, LF_emp, \nlikelihood factors factor estimated data. cases, users\nmay priori knowledge likelihood factor. instance, RCT,\nmight unconditional probability treatment \\(p = 0.5\\).\nAdditional likelihood factor types need create accommodate type \nknowledge.","code":""},{"path":"the-tmle-framework-with-tmle3.html","id":"example-2","chapter":"6 The TMLE Framework with tmle3","heading":"6.7.5 Example","text":"Going back CPP data example, estimate marginal likelihood \n\\(W\\), using NP-MLE, conditional density \\(\\) given \\(W\\) using GLM fit via\nsl3 conditional mean \\(Y\\) given \\(\\) \\(W\\) using another GLM fit\nvia sl3:particular likelihood factors estimation strategies use \ncourse depend parameter interest. list likelihood factors\ndefined, can construct Likelihood object train data\ncontained tmle_task:tmle3 Likelihood actually special type sl3 learner, \nsyntax train data analogous.fit likelihood, can now get likelihood values \ntmle3_Task:","code":"\n# set up sl3 learners for tmle3 fit\nlrnr_glm_fast <- make_learner(Lrnr_glm_fast)\nlrnr_mean <- make_learner(Lrnr_mean)\n\n# define and fit likelihood\nfactor_list <- list(\n  define_lf(LF_emp, \"W\"),\n  define_lf(LF_fit, \"A\", lrnr_glm_fast),\n  define_lf(LF_fit, \"Y\", lrnr_glm_fast, type=\"mean\")\n)\nlikelihood_def <- Likelihood$new(factor_list)\nlikelihood <- likelihood_def$train(tmle_task)\nprint(likelihood)\nW: Lf_emp\nA: LF_fit\nY: LF_fitlikelihood_values <- likelihood$get_likelihoods(tmle_task,\"Y\")\nhead(likelihood_values)\n[1] 0.57930 0.57930 0.69095 0.69095 0.69095 0.45234"},{"path":"the-tmle-framework-with-tmle3.html","id":"counterfactual-likelihoods","chapter":"6 The TMLE Framework with tmle3","heading":"6.8 Counterfactual Likelihoods","text":"tmle3, interventions modeled likelihoods one \nlikelihood factors replaced counterfactual version representing \nintervention.tmle3 defines CF_Likelihood class, inherits Likelihood, \ntakes observed_likelihood intervention_list., describe examples additional likelihood factors intended \nused describe interventions. expect list grow tmle3 \nextended additional use-cases.","code":""},{"path":"the-tmle-framework-with-tmle3.html","id":"lf_static","chapter":"6 The TMLE Framework with tmle3","heading":"6.8.1 LF_static","text":"Likelihood factor static intervention, observations set \nsingle intervention value \\(x'\\):\\[f_{X_j \\mid Pa(X_j)}(x_j \\mid Pa(x_j)) = \\mathbf{}(x_j = x')\\]","code":""},{"path":"the-tmle-framework-with-tmle3.html","id":"other-intervention-likelihood-factor-types","chapter":"6 The TMLE Framework with tmle3","heading":"6.8.2 Other intervention likelihood factor types","text":"Additional likelihood factor types need defined types \ninterventions, dynamic rules stochastic interventions. Currently, \nprototype version stochastic shift intervention exists LF_shift.","code":""},{"path":"the-tmle-framework-with-tmle3.html","id":"example-3","chapter":"6 The TMLE Framework with tmle3","heading":"6.8.3 Example","text":"CPP example, ’ll define simple intervention set \ntreatment \\(= 1\\):can use construct counterfactual likelihood:cf_likelihood likelihood object, behavior \nobserved likelihood object defined , observed likelihood\nfactors replaced defined intervention likelihood factors.particular, can get likelihood values counterfactual likelihood:see likelihood values \\(\\) node either 0 1, \nexpected indicator likelihood function. addition, \nlikelihood values non-intervention nodes changed.","code":"\nintervention <- define_lf(LF_static, \"A\", value = 1)\ncf_likelihood <- make_CF_Likelihood(likelihood, intervention)cf_likelihood_values <- cf_likelihood$get_likelihoods(tmle_task, \"A\")\nhead(cf_likelihood_values)\n[1] 1 1 0 0 0 1"},{"path":"the-tmle-framework-with-tmle3.html","id":"counterfactual-tasks","chapter":"6 The TMLE Framework with tmle3","heading":"6.8.4 Counterfactual Tasks","text":"CF_Likelihood can generate one counterfactual tasks. \ntmle3_Tasks observed values replaced counterfactual values\naccording specified intervention distribution. deterministic\ninterventions, one task generated. However, stochastic\ninterventions, implemented, generate several tasks, one \ncombination possible values intervention node(s).enumerate tasks, use enumerate_cf_tasks:case, can see parity01 set 1 \nobservations, consistent static intervention node.","code":"cf_likelihood_tasks <- cf_likelihood$enumerate_cf_tasks(tmle_task)\nhead(cf_likelihood_tasks[[1]]$data)\n   apgar1 apgar5 gagebrth mage meducyrs sexn parity01 haz01\n1:      8      9      287   21       12    1        1     1\n2:      8      9      287   21       12    1        1     1\n3:      8      9      280   15        0    1        1     1\n4:      8      9      280   15        0    1        1     0\n5:      8      9      280   15        0    1        1     0\n6:      9      9      266   23        0    1        1     1"},{"path":"the-tmle-framework-with-tmle3.html","id":"update-procedure","chapter":"6 The TMLE Framework with tmle3","heading":"6.9 Update Procedure","text":"TMLE framework, define target parameter \\(\\Psi(P)\\) mapping \nprobability distribution \\(P \\\\mathcal{M}\\) set real numbers\n\\(\\mathbb{R}^d\\). \\(\\mathcal{M}\\) implied NPSEM defined .tmle3, define parameter objects objects inheriting \nParam_base class, keep track mapping probability\ndistribution parameter value, also corresponding EIF \nparameter, “clever covariates” needed calculate TMLE update \nlikelihood., define treatment-specific mean (TSM) parameter based \nintervention defined previously:TODO: provide details parameter definition","code":"\ntsm <- define_param(Param_TSM, likelihood, intervention)"},{"path":"the-tmle-framework-with-tmle3.html","id":"update-procedure-1","chapter":"6 The TMLE Framework with tmle3","heading":"6.10 Update Procedure","text":"update procedure component tmle3 currently flux. current\nstructure follows:object, tmle3_Update, calculates individual update steps\nusing tmle3_Update$update_step. adds Likelihood$update_list, \nfuture calls Likelihood$get_likelihoods return updated likelihood\nvalues. However, likelihood values generally recomputed step, \nrequires applying past updates. ridiculously inefficient.Instead, need previous TMLE implementations done, \nenumerate list required likelihood values, update values go\n(opposed updating function recalculating value time \nneeded). requires ability parameters enumerate \nlikelihood values need defining clever covariate, well \nparameter mapping EIF. yet implemented.Therefore, update procedure, well structure Param_base\nparameter objects subject substantial changes near future.Currently, tmle3_Update object also hard-coded submodel (logistic),\nloss function (log-likelihood), solver (GLM). need generalized\nupdates can done range submodels, loss functions, solvers.Current Usage:","code":"\nupdater <- tmle3_Update$new()\ntargeted_likelihood <- Targeted_Likelihood$new(likelihood, updater)"},{"path":"the-tmle-framework-with-tmle3.html","id":"target-parameter","chapter":"6 The TMLE Framework with tmle3","heading":"6.11 Target Parameter","text":"TMLE framework, define target parameter \\(\\Psi(P)\\) mapping \nprobability distribution \\(P \\\\mathcal{M}\\) set real numbers\n\\(\\mathbb{R}^d\\). \\(\\mathcal{M}\\) implied NPSEM defined .tmle3, define parameter objects objects inheriting \nParam_base class, keep track mapping probability\ndistribution parameter value, also corresponding EIF \nparameter, “clever covariates” needed calculate TMLE update \nlikelihood., define treatment specific mean (TSM) parameter based \nintervention defined previously:TODO: provide details parameter definition","code":"\ntsm <- define_param(Param_TSM, likelihood, intervention)\nupdater$tmle_params <- tsm"},{"path":"the-tmle-framework-with-tmle3.html","id":"tmle3_fit---putting-it-all-together","chapter":"6 The TMLE Framework with tmle3","heading":"6.12 tmle3_Fit - Putting it all together","text":"Now specified components required TMLE procedure,\ncan generate object manages components finally calculate\nappropriate TML estimator.","code":"tmle_fit <- fit_tmle3(tmle_task, targeted_likelihood, tsm, updater)\nprint(tmle_fit)\nA tmle3_Fit that took 1 step(s)\n   type      param init_est tmle_est       se   lower   upper psi_transformed\n1:  TSM E[Y_{A=1}]  0.52805  0.52805 0.014644 0.49935 0.55675         0.52805\n   lower_transformed upper_transformed\n1:           0.49935           0.55675"},{"path":"the-tmle-framework-with-tmle3.html","id":"tmle-specification","chapter":"6 The TMLE Framework with tmle3","heading":"6.13 TMLE Specification","text":"tmle3 framework described completely general, allows \ncomponents TMLE procedure specified modular way. However, \nend users interested manually specifying components.\nTherefore, tmle3 implements tmle3_Spec object bundles set \ncomponents specification , minimal additional detail, can \nrun end-user:Currently, effectively hard-coded list details: structure\nNPSEM, parameters, update procedure coded \nspecification. data, roles variables, sl3\nlearners use likelihood estimation. Ideally, instead tmle3_Spec \nrepresent set reasonable defaults particular TMLE, experienced\nusers override appropriate.","code":"nodes <- list(W = c(\"apgar1\", \"apgar5\", \"gagebrth\", \"mage\", \"meducyrs\",\n                    \"sexn\"),\n              A = \"parity01\",\n              Y = \"haz01\")\n\nlrnr_glm_fast <- make_learner(Lrnr_glm_fast)\nlrnr_mean <- make_learner(Lrnr_mean)\nlearner_list <- list(Y = lrnr_mean, A = lrnr_glm_fast)\n\n# make a new copy to deal with data.table weirdness\ncpp2 <- data.table::copy(cpp)\n\ntmle_fit_from_spec <- tmle3(tmle_TSM_all(), cpp2, nodes, learner_list)\nprint(tmle_fit_from_spec)\nA tmle3_Fit that took 1 step(s)\n   type      param init_est tmle_est       se   lower   upper psi_transformed\n1:  TSM E[Y_{A=0}]  0.55517  0.69795 0.046923 0.60598 0.78991         0.69795\n2:  TSM E[Y_{A=1}]  0.55517  0.52674 0.014737 0.49786 0.55562         0.52674\n   lower_transformed upper_transformed\n1:           0.60598           0.78991\n2:           0.49786           0.55562"},{"path":"the-tmle-framework-with-tmle3.html","id":"conclusion","chapter":"6 The TMLE Framework with tmle3","heading":"6.14 Conclusion","text":"Obviously, ’s lot :Generalize tmle3_UpdateGeneralize tmpe3_SpecBetter handling bounded continuous outcomesExpand documentation parameter definitionsAdd support dynamic rules stochastic interventionsCV-TMLEC-TMLEIPCW-TMLEExtension longitudinal data settings","code":""},{"path":"monte-carlo-simulation-with-tmle3sim.html","id":"monte-carlo-simulation-with-tmle3sim","chapter":"7 Monte Carlo Simulation with tmle3sim","heading":"7 Monte Carlo Simulation with tmle3sim","text":"Jeremy Coyle","code":""},{"path":"monte-carlo-simulation-with-tmle3sim.html","id":"intro-4","chapter":"7 Monte Carlo Simulation with tmle3sim","heading":"7.1 Intro","text":"","code":""},{"path":"monte-carlo-simulation-with-tmle3sim.html","id":"architecture-4","chapter":"7 Monte Carlo Simulation with tmle3sim","heading":"7.2 Architecture","text":"","code":""},{"path":"monte-carlo-simulation-with-tmle3sim.html","id":"other-4","chapter":"7 Monte Carlo Simulation with tmle3sim","heading":"7.3 Other","text":"","code":""},{"path":"monte-carlo-simulation-with-tmle3sim.html","id":"previous-documentation-4","chapter":"7 Monte Carlo Simulation with tmle3sim","heading":"7.4 Previous Documentation","text":"","code":"\nlibrary(devtools)\nlibrary(tmle3)\n\nload_all()\n\n# define simulation / DGD\n# TODO: maybe DGD should be separate object\nexample_dgd <- function(n, mean, sd, ...) {\n  list(x = rnorm(n, mean, sd))\n}\n\nparams <- list(\n  n = 1000,\n  mean = 0,\n  sd = 1\n)\n\nsimulation <- sim_from_fun(example_dgd,\n  params = params,\n  vebose = TRUE\n)\n\n# define estimation strategy\n\nexample_est <- function(simulation, ...) {\n  data <- simulation$last_sample\n  result <- list(\n    xbar = mean(data$x),\n    sigma = sd(data$x)\n  )\n\n  return(result)\n}\n\nexample_est2 <- function(simulation, ...) {\n  data <- simulation$last_sample\n  result <- list(\n    xbar = median(data$x),\n    sigma = IQR(data$x)\n  )\n\n  return(result)\n}\n\nmean_est <- est_from_fun(example_est, params = list(name = \"mean_estimator\"))\nmedian_est <- est_from_fun(example_est2, params = list(name = \"median_estimator\"))\n\nt3s_Reporter$new()\n\nsimulations <- list(simululation)\nsimulation$estimator <-\n  simulation$reporter <-\n  simulation$full_params\n# debugonce(simulation$reporter$report)\n# debugonce(simulation$reporter$make_final)\n\nsimulation$run()\ndebugonce(simulation$reporter$save)\nsimulation$reporter$save()"},{"path":"highly-adaptive-lasso-with-hal9001.html","id":"highly-adaptive-lasso-with-hal9001","chapter":"8 Highly Adaptive Lasso with hal9001","heading":"8 Highly Adaptive Lasso with hal9001","text":"Jeremy Coyle","code":""},{"path":"highly-adaptive-lasso-with-hal9001.html","id":"intro-5","chapter":"8 Highly Adaptive Lasso with hal9001","heading":"8.1 Intro","text":"","code":""},{"path":"highly-adaptive-lasso-with-hal9001.html","id":"architecture-5","chapter":"8 Highly Adaptive Lasso with hal9001","heading":"8.2 Architecture","text":"","code":""},{"path":"highly-adaptive-lasso-with-hal9001.html","id":"other-5","chapter":"8 Highly Adaptive Lasso with hal9001","heading":"8.3 Other","text":"","code":""},{"path":"highly-adaptive-lasso-with-hal9001.html","id":"previous-documentation-5","chapter":"8 Highly Adaptive Lasso with hal9001","heading":"8.4 Previous Documentation","text":"","code":""},{"path":"highly-adaptive-lasso-with-hal9001.html","id":"introduction-2","chapter":"8 Highly Adaptive Lasso with hal9001","heading":"8.5 Introduction","text":"highly adaptive Lasso (HAL) flexible machine learning algorithm \nnonparametrically estimates function based available data embedding \nset input observations covariates extremely high-dimensional space\n(.e., generating basis functions available data). input data\nmatrix \\(n\\) observations \\(d\\) covariates, number basis functions\ngenerated approximately \\(n \\cdot 2^{d - 1}\\). select set basis\nfunctions among full set generated, Lasso employed. \nhal9001 R package provides efficient implementation routine,\nrelying glmnet R package compatibility canonical Lasso\nimplementation still providing (faster) custom C++ routine using \nLasso input matrix composed indicator functions. Consider consulting\nBenkeser van der Laan (2016), (???), (???) detailed theoretical\ndescriptions highly adaptive Lasso various optimality properties.TODO: port documentation https://github.com/tlverse/hal9001/blob/master/vignettes/intro_hal9001.Rmd","code":""},{"path":"contributing-to-the-tlverse.html","id":"contributing-to-the-tlverse","chapter":"9 Contributing to the tlverse","heading":"9 Contributing to the tlverse","text":"Jeremy CoyleTODO: generalize","code":""},{"path":"contributing-to-the-tlverse.html","id":"submitting-your-learner-to-sl3","chapter":"9 Contributing to the tlverse","heading":"9.1 Submitting your Learner to sl3","text":"’ve implemented new learner (made sure quality\ndocumentation unit tests), please consider adding sl3 project.\nmake possible sl3 users use build work.\nMake sure add R packages listed .required_packages \nSuggests: field DESCRIPTION file sl3 package. \ndone, please submit Pull Request sl3 package \nGitHub request learned \nadded. ’ve never made “Pull Request” , see helpful\nguide: https://yangsu.github.io/pull-request-tutorial/.tlverse team, thanks interest extending tlverse!","code":""},{"path":"contributing-to-the-tlverse.html","id":"contributing-to-sl3-development","chapter":"9 Contributing to the tlverse","heading":"9.2 Contributing to sl3 development","text":", authors sl3 R package, use guide used \ncontributing development popular ggplot2 R package. \ndocument simply formal re-statement fact.goal guide help get contributing sl3 \nquickly possible. guide divided two main pieces:Filing bug report feature request issue.Suggesting change via pull request.","code":""},{"path":"contributing-to-the-tlverse.html","id":"issues","chapter":"9 Contributing to the tlverse","heading":"9.3 Issues","text":"filing issue, important thing include minimal\nreproducible example can quickly verify problem, figure\nfix . three things need include make \nexample reproducible: required packages, data, code.Packages loaded top script, ’s easy \nsee ones example needs.Packages loaded top script, ’s easy \nsee ones example needs.easiest way include data use dput() generate R\ncode recreate . example, recreate mtcars dataset R,\n’d perform following steps:\nRun dput(mtcars) R\nCopy output\nreproducible script, type mtcars <- paste.\neven better can create data.frame() just handful\nrows columns still illustrates problem.easiest way include data use dput() generate R\ncode recreate . example, recreate mtcars dataset R,\n’d perform following steps:Run dput(mtcars) RCopy outputIn reproducible script, type mtcars <- paste.even better can create data.frame() just handful\nrows columns still illustrates problem.Spend little bit time ensuring code easy others \nread:\nmake sure ’ve used spaces variable names concise, \ninformative\nuse comments indicate problem lies\nbest remove everything related problem.\nshorter code , easier understand.\nSpend little bit time ensuring code easy others \nread:make sure ’ve used spaces variable names concise, \ninformativemake sure ’ve used spaces variable names concise, \ninformativeuse comments indicate problem liesuse comments indicate problem liesdo best remove everything related problem.\nshorter code , easier understand.best remove everything related problem.\nshorter code , easier understand.can check actually made reproducible example starting \nfresh R session pasting script .(Unless ’ve specifically asked , please don’t include output\nsessionInfo().)","code":""},{"path":"contributing-to-the-tlverse.html","id":"pull-requests","chapter":"9 Contributing to the tlverse","heading":"9.4 Pull requests","text":"contribute change sl3, follow steps:Create branch git make changes.Push branch github issue pull request (PR).Discuss pull request.Iterate either accept PR decide ’s good fit \nsl3.steps described detail . might feel\noverwhelming first time get set , gets easier practice.’re familiar git GitHub, please start reading\nhttp://r-pkgs..co.nz/git.htmlPull requests evaluated checklist:Motivation. pull request clearly concisely motivates \nneed change. Plesae describe problem PR addresses show\npull request solves concisely possible.Also include motivation NEWS new release \nggplot2 comes ’s easy users see ’s changed. Add \nitem top file use markdown formatting. \nnews item end (@yourGithubUsername, #the_issue_number).related changes. submit pull request, please\ncheck make sure haven’t accidentally included unrelated\nchanges. make harder see exactly ’s changed, \nevaluate unexpected side effects.\nPR corresponds git branch, expect submit\nmultiple changes make sure create multiple branches. \nmultiple changes depend , start first one\ndon’t submit others first one processed.related changes. submit pull request, please\ncheck make sure haven’t accidentally included unrelated\nchanges. make harder see exactly ’s changed, \nevaluate unexpected side effects.PR corresponds git branch, expect submit\nmultiple changes make sure create multiple branches. \nmultiple changes depend , start first one\ndon’t submit others first one processed.Use sl3 coding style. Please follow \nofficial ggplot2 style. Maintaing\nconsistent style across whole code base makes much easier \njump code. ’re modifying existing ggplot2 code \ndoesn’t follow style guide, separate pull request fix \nstyle greatly appreciated.Use sl3 coding style. Please follow \nofficial ggplot2 style. Maintaing\nconsistent style across whole code base makes much easier \njump code. ’re modifying existing ggplot2 code \ndoesn’t follow style guide, separate pull request fix \nstyle greatly appreciated.’re adding new parameters new function, ’ll also need\ndocument roxygen.\nMake sure re-run devtools::document() code submitting.’re adding new parameters new function, ’ll also need\ndocument roxygen.\nMake sure re-run devtools::document() code submitting.seems like lot work don’t worry pull request isn’t\nperfect. ’s learning process. pull request process, unless\n’ve submitted past ’s unlikely pull request \naccepted . Please don’t submit pull requests change existing\nbehaviour. Instead, think can add new feature minimally\ninvasive way.","code":""},{"path":"ensuring-code-quality.html","id":"ensuring-code-quality","chapter":"10 Ensuring Code Quality","heading":"10 Ensuring Code Quality","text":"Jeremy Coyle","code":""},{"path":"ensuring-code-quality.html","id":"doctest","chapter":"10 Ensuring Code Quality","heading":"10.1 Documenting and Testing your Learner","text":"TODO: generalizeIf want people able use learner, need \ndocument provide unit tests . template example\ndocumentation, written roxygen\nformat. importantly, describe learner , reference\nexternal code uses, document parameters public methods\ndefined .’s also important test learner.\nwrite unit tests verify learner can train predict \nnew data, , applicable, generate chained task. might also good\nidea use risk function sl3 verify learner’s performance \nsample dataset. way, change learner performance drops,\nknow something may gone wrong.","code":""},{"path":"references.html","id":"references","chapter":"References","heading":"References","text":"Benkeser, David, Mark J van der Laan. 2016. “Highly Adaptive Lasso Estimator.” 2016 IEEE International Conference Data Science Advanced Analytics (DSAA). IEEE. https://doi.org/10.1109/dsaa.2016.93.Coyle, Jeremy R, Nima S Hejazi. 2018. “Origami: Generalized Framework Cross-Validation R.” Journal Open Source Software 3 (21). https://doi.org/10.21105/joss.00512.","code":""}]
